# This script gathers all the relevant data from the Dortmund Data Bank

import os
import pandas as pd
import requests
import time

from bs4 import BeautifulSoup


def get_all_urls() -> list:
    """
    Gets all the urls to the different pairs of solutes-solvents

    :return: A list of all the urls
    """
    all_links_url = base_url + "ACTindex.php"
    link_ends = []
    r = requests.get(all_links_url)  # Get the raw html from the website
    soup = BeautifulSoup(r.content, "html.parser")  # Parse the html
    a_tag = soup.find_all("a", href=True)  # Get all hyperlinks on the page
    for link in a_tag:
        # The only hyperlinks we need are ones with ACT in them, all others are irrelevant
        if "ACT " in link["href"]:
            link_end = link["href"].replace(" ", "%20")
            # Only add new links because some links are repeated
            if link_end not in link_ends:
                link_ends.append(link_end)
    return link_ends


def get_tables_in_link(link: str) -> list:
    """
    Gets all the relevant tables on a page and puts the table data into a list

    :param link: The link to the website with all the necessary data
    :return: A list of all the relevant table data
    """
    r = requests.get(link.strip())  # Get the raw html from the website
    soup = BeautifulSoup(r.content, "html.parser")  # Parse the html
    tables = soup.find_all("table")  # Get all the tables on the page
    table_data = []
    for table in tables:
        # This will filter out all the irrelevant tables
        if "Temperature [K]" in table.text or "CAS Registry Number" in table.text:
            data = []  # Initialize an empty list
            body = table.find("tbody")
            rows = body.find_all("tr")
            # Parse through the rows of the table and grab the information from them
            for row in rows:
                column = row.find_all("td")
                columns = [data.text.strip() for data in column]
                data.append(columns)
            table_data.append(data)
    return table_data


def get_data_from_tables(page_data: list, full_data: list):
    """
    Cleans up the data from the get_tables_in_link method

    :param page_data: The list of data generated by get_tables_in_link
    :param full_data: An empty list for the clean data to go
    :return: A formatted list with each item being a list containing the solvent, solute, and activity coefficient
    """
    solvent = page_data[0][1][-1]  # Gets the solvent name from the page
    solute = page_data[0][2][-1]  # Gets the solute name from the page
    sol_sol_data = page_data[1][1:]  # Gets the activity coefficient from the page
    full_data.append([solvent, solute, sol_sol_data])  # Adds a list of the data gathered to the full_data
    print(f"Added {solvent}-{solute} data")
    # If there are three tables, meaning 2 different activity coefficient tables, then we need to get that second table
    # The second table will always be the one where the solvent and solute are flipped because it is a binary mixture
    if len(page_data) == 3:
        sol_sol_data1 = page_data[2][1:]
        full_data.append([solute, solvent, sol_sol_data1])  # Adds the data gathered from the second table to full_data
        print(f"Added {solute}-{solvent} data")


def filter_data(unfiltered: list) -> list:
    """
    Filters the clean data to contain only data for activity coefficients for mixtures mixed at 298K(±1K)

    :param unfiltered: The cleaned data from Dortmund Data Bank
    :return: A list of the filtered data containing only activity coefficients for mixtures mixed at 298(±1K)
    """
    filtered = []
    for i in unfiltered:
        for j in i[2]:
            if 297.15 <= float(j[0]) <= 299.15:
                filt = [i[0], i[1], float(j[1])]
                filtered.append(filt)
    return filtered


def main():
    # Checks to see if there is a links folder containing all the links to the websites containing data
    # This is to save time when running the program multiple times because the output will always be the same
    if not os.path.exists("./links.txt"):
        # If the file doesn't exist, meaning the script hasn't been run yet, then create the file with all the links
        with open("links.txt", "w") as f:
            for partial in get_all_urls():
                f.write(f"{base_url}{partial}\n")
    data = []
    # Gets the data for each of the links in the links file
    with open("links.txt", "r") as f:
        for link in f.readlines():
            get_data_from_tables(get_tables_in_link(link), data)
            time.sleep(0.5)  # For making sure I don't get locked out from the site

    filtered_data = filter_data(data)  # Filters the clean data
    df = pd.DataFrame(filtered_data, columns=['Solvent', 'Solute', 'Activity Coeff. at Inf. Dil.'])
    df.to_csv("./Solvent-Solute-Activity-Coeff.csv", index=False)
    print("CSV file created!")


if __name__ == '__main__':
    base_url = "http://www.ddbst.com/en/EED/ACT/"  # This a global variable to create all the urls that will be used
    main()
